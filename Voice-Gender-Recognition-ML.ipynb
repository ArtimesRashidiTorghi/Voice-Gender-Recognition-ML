{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Artimes Rashidi Torghi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2. Gender Recognition by Speech Analysis\n",
    "\n",
    "\n",
    "**FOR ALL MODELS THAT REQUIRES RANDOM STATE, SPECIFY RANDOM STATE TO 0**\n",
    "\n",
    "**IF HYPERPARAMETER IS NOT SPECIFIED, LEAVE AS DEFAULT**\n",
    "\n",
    "\n",
    "#### Setting and Data\n",
    "This dataset is created to identify a voice as male or female, based upon acoustic properties of the voice and speech. It consists of 3,168 recorded voice samples, collected from male and female speakers. The voice samples are pre-processed by acoustic analysis, with an analyzed frequency range of 0hz-280hz (human vocal range).\n",
    "\n",
    "The CSV file contains 20 acoustic properties of each voice, and one outcome variable, “label”, which identifies the gender of the speaker. The detailed information is listed below (you do NOT need to read through the variable description). \n",
    "\n",
    "- meanfreq: mean frequency (in kHz)\n",
    "- sd: standard deviation of frequency\n",
    "- median: median frequency (in kHz)\n",
    "- Q25: first quantile (in kHz)\n",
    "- Q75: third quantile (in kHz)\n",
    "- IQR: interquantile range (in kHz)\n",
    "- skew: skewness (see note in specprop description)\n",
    "- kurt: kurtosis (see note in specprop description)\n",
    "- sp.ent: spectral entropy\n",
    "- sfm: spectral flatness\n",
    "- mode: mode frequency\n",
    "- centroid: frequency centroid (see specprop)\n",
    "- meanfun: average of fundamental frequency measured across acoustic signal\n",
    "- minfun: minimum fundamental frequency measured across acoustic signal\n",
    "- maxfun: maximum fundamental frequency measured across acoustic signal\n",
    "- meandom: average of dominant frequency measured across acoustic signal\n",
    "- mindom: minimum of dominant frequency measured across acoustic signal\n",
    "- maxdom: maximum of dominant frequency measured across acoustic signal\n",
    "- dfrange: range of dominant frequency measured across acoustic signal\n",
    "- modindx: modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range\n",
    "- label: male or female\n",
    "\n",
    "#### Data Preparation\n",
    "Use the code below to load data and check the variable names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.180907</td>\n",
       "      <td>0.057126</td>\n",
       "      <td>0.185621</td>\n",
       "      <td>0.140456</td>\n",
       "      <td>0.224765</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>3.140168</td>\n",
       "      <td>36.568461</td>\n",
       "      <td>0.895127</td>\n",
       "      <td>0.408216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180907</td>\n",
       "      <td>0.142807</td>\n",
       "      <td>0.036802</td>\n",
       "      <td>0.258842</td>\n",
       "      <td>0.829211</td>\n",
       "      <td>0.052647</td>\n",
       "      <td>5.047277</td>\n",
       "      <td>4.994630</td>\n",
       "      <td>0.173752</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.029918</td>\n",
       "      <td>0.016652</td>\n",
       "      <td>0.036360</td>\n",
       "      <td>0.048680</td>\n",
       "      <td>0.023639</td>\n",
       "      <td>0.042783</td>\n",
       "      <td>4.240529</td>\n",
       "      <td>134.928661</td>\n",
       "      <td>0.044980</td>\n",
       "      <td>0.177521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029918</td>\n",
       "      <td>0.032304</td>\n",
       "      <td>0.019220</td>\n",
       "      <td>0.030077</td>\n",
       "      <td>0.525205</td>\n",
       "      <td>0.063299</td>\n",
       "      <td>3.521157</td>\n",
       "      <td>3.520039</td>\n",
       "      <td>0.119454</td>\n",
       "      <td>0.500079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.039363</td>\n",
       "      <td>0.018363</td>\n",
       "      <td>0.010975</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.042946</td>\n",
       "      <td>0.014558</td>\n",
       "      <td>0.141735</td>\n",
       "      <td>2.068455</td>\n",
       "      <td>0.738651</td>\n",
       "      <td>0.036876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039363</td>\n",
       "      <td>0.055565</td>\n",
       "      <td>0.009775</td>\n",
       "      <td>0.103093</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.163662</td>\n",
       "      <td>0.041954</td>\n",
       "      <td>0.169593</td>\n",
       "      <td>0.111087</td>\n",
       "      <td>0.208747</td>\n",
       "      <td>0.042560</td>\n",
       "      <td>1.649569</td>\n",
       "      <td>5.669547</td>\n",
       "      <td>0.861811</td>\n",
       "      <td>0.258041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163662</td>\n",
       "      <td>0.116998</td>\n",
       "      <td>0.018223</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.419828</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>2.070312</td>\n",
       "      <td>2.044922</td>\n",
       "      <td>0.099766</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.184838</td>\n",
       "      <td>0.059155</td>\n",
       "      <td>0.190032</td>\n",
       "      <td>0.140286</td>\n",
       "      <td>0.225684</td>\n",
       "      <td>0.094280</td>\n",
       "      <td>2.197101</td>\n",
       "      <td>8.318463</td>\n",
       "      <td>0.901767</td>\n",
       "      <td>0.396335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184838</td>\n",
       "      <td>0.140519</td>\n",
       "      <td>0.046110</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.765795</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>4.992188</td>\n",
       "      <td>4.945312</td>\n",
       "      <td>0.139357</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.199146</td>\n",
       "      <td>0.067020</td>\n",
       "      <td>0.210618</td>\n",
       "      <td>0.175939</td>\n",
       "      <td>0.243660</td>\n",
       "      <td>0.114175</td>\n",
       "      <td>2.931694</td>\n",
       "      <td>13.648905</td>\n",
       "      <td>0.928713</td>\n",
       "      <td>0.533676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199146</td>\n",
       "      <td>0.169581</td>\n",
       "      <td>0.047904</td>\n",
       "      <td>0.277457</td>\n",
       "      <td>1.177166</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>7.007812</td>\n",
       "      <td>6.992188</td>\n",
       "      <td>0.209183</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.251124</td>\n",
       "      <td>0.115273</td>\n",
       "      <td>0.261224</td>\n",
       "      <td>0.247347</td>\n",
       "      <td>0.273469</td>\n",
       "      <td>0.252225</td>\n",
       "      <td>34.725453</td>\n",
       "      <td>1309.612887</td>\n",
       "      <td>0.981997</td>\n",
       "      <td>0.842936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251124</td>\n",
       "      <td>0.237636</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>0.279114</td>\n",
       "      <td>2.957682</td>\n",
       "      <td>0.458984</td>\n",
       "      <td>21.867188</td>\n",
       "      <td>21.843750</td>\n",
       "      <td>0.932374</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          meanfreq           sd       median          Q25          Q75  \\\n",
       "count  3168.000000  3168.000000  3168.000000  3168.000000  3168.000000   \n",
       "mean      0.180907     0.057126     0.185621     0.140456     0.224765   \n",
       "std       0.029918     0.016652     0.036360     0.048680     0.023639   \n",
       "min       0.039363     0.018363     0.010975     0.000229     0.042946   \n",
       "25%       0.163662     0.041954     0.169593     0.111087     0.208747   \n",
       "50%       0.184838     0.059155     0.190032     0.140286     0.225684   \n",
       "75%       0.199146     0.067020     0.210618     0.175939     0.243660   \n",
       "max       0.251124     0.115273     0.261224     0.247347     0.273469   \n",
       "\n",
       "               IQR         skew         kurt       sp.ent          sfm  ...  \\\n",
       "count  3168.000000  3168.000000  3168.000000  3168.000000  3168.000000  ...   \n",
       "mean      0.084309     3.140168    36.568461     0.895127     0.408216  ...   \n",
       "std       0.042783     4.240529   134.928661     0.044980     0.177521  ...   \n",
       "min       0.014558     0.141735     2.068455     0.738651     0.036876  ...   \n",
       "25%       0.042560     1.649569     5.669547     0.861811     0.258041  ...   \n",
       "50%       0.094280     2.197101     8.318463     0.901767     0.396335  ...   \n",
       "75%       0.114175     2.931694    13.648905     0.928713     0.533676  ...   \n",
       "max       0.252225    34.725453  1309.612887     0.981997     0.842936  ...   \n",
       "\n",
       "          centroid      meanfun       minfun       maxfun      meandom  \\\n",
       "count  3168.000000  3168.000000  3168.000000  3168.000000  3168.000000   \n",
       "mean      0.180907     0.142807     0.036802     0.258842     0.829211   \n",
       "std       0.029918     0.032304     0.019220     0.030077     0.525205   \n",
       "min       0.039363     0.055565     0.009775     0.103093     0.007812   \n",
       "25%       0.163662     0.116998     0.018223     0.253968     0.419828   \n",
       "50%       0.184838     0.140519     0.046110     0.271186     0.765795   \n",
       "75%       0.199146     0.169581     0.047904     0.277457     1.177166   \n",
       "max       0.251124     0.237636     0.204082     0.279114     2.957682   \n",
       "\n",
       "            mindom       maxdom      dfrange      modindx        label  \n",
       "count  3168.000000  3168.000000  3168.000000  3168.000000  3168.000000  \n",
       "mean      0.052647     5.047277     4.994630     0.173752     0.500000  \n",
       "std       0.063299     3.521157     3.520039     0.119454     0.500079  \n",
       "min       0.004883     0.007812     0.000000     0.000000     0.000000  \n",
       "25%       0.007812     2.070312     2.044922     0.099766     0.000000  \n",
       "50%       0.023438     4.992188     4.945312     0.139357     0.500000  \n",
       "75%       0.070312     7.007812     6.992188     0.209183     1.000000  \n",
       "max       0.458984    21.867188    21.843750     0.932374     1.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "voice = pd.read_csv('voice.csv') \n",
    "voice['label']=voice['label'].astype('category').cat.codes\n",
    "voice.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to use all other variables to predict the gender of the speaker (label). To start, we prepare the data for analysis using code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = voice.iloc[:,0:19]\n",
    "y = voice.iloc[:,20]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1: Scaling and Basic Models ** \n",
    "\n",
    "- Use standard scaler to scale the data, so that the data can be applied for **supervised learning models**.\n",
    "- Based on the data description, choose the proper Naive Bayes model. Report the training and test accuracy. \n",
    "    - If we train the same NB model on unscaled data, do you expect a performance change of the model? If yes, briefly explain how the performance will change. If no, provide explanations. [Discussion Only]\n",
    "- Train a Decision Tree model with maximum depth = 2. Report the training and test accuracy. \n",
    "    - If we train the same DT model on the same, scaled training set, but add an additional parameter, where maximum leaf node is 6. Do you expect a performance change of the model? If yes, briefly explain how the performance will change. If no, provide explanations. [Discussion Only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.6683840596883445e-16\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# S1: Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print( np.mean(X_train_scaled))\n",
    "print(np.var(X_train_scaled))\n",
    "# Standard scaler transfer X to mean = 0, s.d. = var = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8926767676767676, 0.8943602693602694)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "g_nb = GaussianNB()\n",
    "g_nb.fit(X_train_scaled, y_train)\n",
    "g_nb.score(X_test_scaled, y_test),g_nb.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No it is not influesnce because scaling is not important in naive bayes(it is based on the probability and similality\n",
    "#betweeen records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3, random_state=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let maximum depth be 3\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# A Basic Tree\n",
    "tree_3 = DecisionTreeClassifier(random_state = 0, max_depth = 3)\n",
    "\n",
    "tree_3.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9558080808080808, 0.9718013468013468)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_3.score(X_test_scaled, y_test), tree_3.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for sure we can say we will have better accuracy in training set,but when we increase the max depth\n",
    "#it is possible to have overfittiong problem but always larger tree lead to larger accuracy\n",
    "#so it would better to include both fit and over fit for evaluationg\n",
    "#, but 6 is not that much and we will have better accuracy in both test \n",
    "#and train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2: Linear and Kernel SVM ** \n",
    "\n",
    "- Train a linear SVM classifier with C = 1. Report the training and test accuracy. \n",
    "\n",
    "- Tune a kernel SVM classifier. Let C be 10^k, where k are integers from -1 to 2, inclusive. Consider two kernel functions: polynomial and gaussian, each with their default hyperparameter specification.\n",
    "    - What is the optimal C and kernel function?\n",
    "    - Report the training and test accuracy of the best model. \n",
    "\n",
    "- If we train another kernel SVM classifier with C = 1000 (same kernel as the chosen model) and default hyperparameter. Without training the new model, do you expect its training accuracy to be higher or lower than the previous (chosen) kernel SVM model? Explain briefly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9747474747474747, 0.9718013468013468)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear SVC \n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lr_svm = LinearSVC(random_state = 0,C = 1)\n",
    "lr_svm.fit(X_train_scaled, y_train)\n",
    "lr_svm.score(X_test_scaled, y_test), lr_svm.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define Function\n",
    "base_svm = SVC(random_state = 0, kernel = 'rbf')\n",
    "\n",
    "#define a list of parameters\n",
    "param_svc_kernel = {'C':  [ 0.1, 1, 10, 100]  } # C = 10,000 mimics hard-margin SVM\n",
    "\n",
    "#apply grid search\n",
    "grid_svm = GridSearchCV(base_svm, param_svc_kernel, cv = 5, n_jobs=2)\n",
    "\n",
    "grid_svm.fit(X_train_scaled, y_train)\n",
    "grid_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9835858585858586, 0.9886363636363636)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_kernel_basicc = SVC(random_state = 0, kernel = 'rbf',C=10)\n",
    "svc_kernel_basicc.fit(X_train_scaled, y_train)\n",
    "svc_kernel_basicc.score(X_test_scaled, y_test),svc_kernel_basic.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define Function\n",
    "base_svm = SVC(random_state = 0, kernel = 'poly')\n",
    "\n",
    "#define a list of parameters\n",
    "param_svc_kernel = {'C':  [ 0.1, 1, 10, 100]  } # C = 10,000 mimics hard-margin SVM\n",
    "\n",
    "#apply grid search\n",
    "grid_svm = GridSearchCV(base_svm, param_svc_kernel, cv = 5, n_jobs=2)\n",
    "\n",
    "grid_svm.fit(X_train_scaled, y_train)\n",
    "grid_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9747474747474747, 0.9886363636363636)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_kernel_basic = SVC(random_state = 0, kernel = 'poly',C=10)\n",
    "svc_kernel_basic.fit(X_train_scaled, y_train)\n",
    "svc_kernel_basic.score(X_test_scaled, y_test),svc_kernel_basic.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definatly we capture the noise and lead to overfitting,because we are near to hard margin and\n",
    "#do not let the points be in the street(not tilerant to misclasification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3: Ensemble Method I **\n",
    "\n",
    "For now, let us focus on the models we have trained: Decision Tree, Naïve Bayes, Linear SVM, and Kernel SVM.\n",
    "- Use voting classifier (with hard voting method) that includes the above-mentioned FOUR models. For kernel SVM, use the optimal model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score for voting classifier is: 0.9747474747474747\n",
      "Train score for voting classifier is: 0.9797979797979798\n"
     ]
    }
   ],
   "source": [
    "# S2: Apply Voting Classifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# define voting classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('svm', svc_kernel_basicc), ('DT', tree_3), ('naive', g_nb)],voting='hard')\n",
    "\n",
    "\n",
    "# VotingClassifier(\n",
    "#     estimators = [ ('lr', log_clf)  , ('svm', svm_clf), ('g_nb', nbg_clf) ], \n",
    "#     voting = 'hard')\n",
    "\n",
    "\n",
    "# train the model\n",
    "voting_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Performance Measure\n",
    "print(\"Test score for voting classifier is:\", voting_clf.score(X_test_scaled, y_test))\n",
    "print(\"Train score for voting classifier is:\", voting_clf.score(X_train_scaled, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no it does not,first of all, ensemble method ddoes not guarantee better performance\n",
    "#second, when we have a well performed model such as svm with high accuracy with anothers weak learner\n",
    "#it is obvious that voting less likely to help "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it would be better than the hard voting but i think it does not help that much\n",
    "#it is better than hard voting because it does not behave for example with svm and naive similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9633838383838383\n",
      "0.9608585858585859\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=200, random_state=0,max_depth = 2)\n",
    "rnd_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(rnd_clf.score(X_test_scaled, y_test))\n",
    "print(rnd_clf.score(X_train_scaled, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9835858585858586\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Define base model\n",
    "naive_dt = DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "# AdaBoost\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    naive_dt, n_estimators=200,random_state=0)\n",
    "\n",
    "ada_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Performance\n",
    "print(ada_clf.score(X_test_scaled, y_test))\n",
    "print(ada_clf.score(X_train_scaled, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest is based on voting clasiffieers and bagging with help of the decision tree\n",
    "#the randomness come from different trainnig sample and different attributes and use majority vote for prediction\n",
    "#ada boost is based on two ideas: 1) points are not equal and 2)method are not equal and some of them are more accurate\n",
    "#prediction is based on weighted combination of the prediction of k individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the data contain around halph male and halph female voices means around 50% male and 50 % female\n",
    "#if the data can predict 51% of the voices is female in test and 49% male in test set it was good but\n",
    "# here it is predict 50% of the women voices which is not that good,\n",
    "#51% accuracy is not good at all, if all of the weak learners aad 51% and they were independent from each other,\n",
    "#ensemble method is more likely to help, but if after implementnd ensemble method \n",
    "#accuracy is 51%, it is not good "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5: Clustering (10 pts)**\n",
    "\n",
    "I Consider another scenario where only collect the acoustic information and fail to collect the gender information (e.g., in an online setting, some users prefer not to disclose their gender). This case, I'll consider using clustering methods to find groups of acoustic information similar to each other. \n",
    "- Use standard scaler to scale the data, so that the data can be applied for **unsupervised learning models.**\n",
    "- Train a DBSCAN model with MinPts = 5 and epsilon = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S1: Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 ...  0  0  0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S2: Apply DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=3, min_samples=5)\n",
    "\n",
    "clusters = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "print(clusters)\n",
    "\n",
    "np.max(clusters)\n",
    "\n",
    "# 2 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 2 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=2, random_state=0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S2: K-Means Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km_3 =  KMeans(n_clusters = 2, random_state = 0)\n",
    "\n",
    "km_3.fit(X_scaled, y)\n",
    "\n",
    "# print(\"Cluster Centers: \\n\", )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: [0 0 0 ... 1 1 0]\n",
      "Performance Score: -41046.18784171887\n"
     ]
    }
   ],
   "source": [
    "# Predict the class labels\n",
    "\n",
    "cls_predict = km_3.predict(X)\n",
    "# cls_predict\n",
    "print(\"Predicted Labels:\", cls_predict)\n",
    "print(\"Performance Score:\", km_3.score(X_scaled, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yes, db scan exaclty give us two clusters which is male and female \n",
    "#we specify k =2 because we know we have two clusters(malae female) so now kmean seperate it to two clusters\n",
    "#if we have out liers dbscan can perform better than the kmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** PCA **\n",
    "\n",
    "In this question, I  conduct dimension reduction to our features using PCA.\n",
    "- Apply PCA to original features (without scaling) and keep 4 components. \n",
    "    - Report: (1) the explained variance ratio of the first four components, and (2) the coefficients to obtain the first component.\n",
    "\n",
    "- Apply PCA to features after standard scaling (i.e., data after scaling in Question 6) and keep 4 components. \n",
    "    - Report: (1) the explained variance ratio of the first four components, and (2) the coefficients to obtain the first component.\n",
    "\n",
    "- Now compare the two PCA methods, and answer questions below.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transfer, the dimension is: 19 \n",
      " After transfer, the dimension is: 4\n"
     ]
    }
   ],
   "source": [
    "# S1: Apply PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_2 = PCA(n_components = 4, whiten = True, random_state = 0)\n",
    "\n",
    "X2D = pca_2.fit_transform(X)\n",
    "\n",
    "print(\"Before transfer, the dimension is:\", X.shape[1], \"\\n\",\n",
    "      \"After transfer, the dimension is:\", X2D.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999977243489689"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S2: Finding the \"explained variance\", or the information kept after transfer\n",
    "\n",
    "pca_2.explained_variance_ratio_\n",
    "\n",
    "# \n",
    "\n",
    "np.sum(pca_2.explained_variance_ratio_)\n",
    "\n",
    "# keep 99% of total info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.00771873e-05,  4.27241447e-05, -6.55942643e-05,\n",
       "        -1.26314510e-04, -2.60997882e-05,  1.00214722e-04,\n",
       "         3.06914989e-02,  9.99477060e-01, -4.25065635e-05,\n",
       "         1.44660905e-04, -2.32694432e-04, -7.00771873e-05,\n",
       "        -4.65719108e-05, -2.89465815e-05, -1.02112765e-05,\n",
       "        -1.18093496e-03, -4.84225941e-05, -7.16975128e-03,\n",
       "        -7.12132868e-03],\n",
       "       [ 2.79533266e-03, -1.38316650e-03,  2.92352677e-03,\n",
       "         3.80016113e-03,  1.51786755e-03, -2.28229358e-03,\n",
       "        -3.56027611e-02,  1.12633067e-02, -3.45238806e-03,\n",
       "        -1.55022124e-02,  6.10411420e-03,  2.79533266e-03,\n",
       "         1.55657922e-03,  1.09225993e-03,  2.25184112e-03,\n",
       "         8.31519275e-02, -1.49915783e-04,  7.03965197e-01,\n",
       "         7.04115112e-01],\n",
       "       [-9.03035639e-04,  3.67640770e-03,  6.16935537e-04,\n",
       "        -1.01844232e-02,  6.10644258e-03,  1.62908658e-02,\n",
       "        -9.96773616e-01,  3.02550823e-02,  2.10302175e-02,\n",
       "         4.40278482e-02,  8.97860624e-03, -9.03035639e-04,\n",
       "        -5.62968110e-03,  7.43694967e-04,  3.51041474e-03,\n",
       "         3.01170015e-02, -1.33915123e-02, -3.33708299e-02,\n",
       "        -1.99793176e-02],\n",
       "       [-2.30348639e-02,  1.12730926e-02, -2.44921271e-02,\n",
       "        -3.29826654e-02, -1.15123338e-02,  2.14703317e-02,\n",
       "        -2.53462478e-02,  4.23144546e-04,  2.81608804e-02,\n",
       "         1.31751930e-01, -4.21772490e-02, -2.30348639e-02,\n",
       "        -1.13370964e-02, -1.29625623e-02, -7.94396994e-03,\n",
       "        -9.83007941e-01, -3.85569809e-02,  4.01483623e-02,\n",
       "         7.87053432e-02]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_2.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xnew = x1 -7.00771873e-05+ x2  4.27241447e-05+ x3 -6.55942643e-05+ x4-1.26314510e-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transfer, the dimension is: 19 \n",
      " After transfer, the dimension is: 4\n"
     ]
    }
   ],
   "source": [
    "# S1: Apply PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_ = PCA(n_components = 4, whiten = True, random_state = 0)\n",
    "\n",
    "X2D = pca_.fit_transform(X_scaled)\n",
    "\n",
    "print(\"Before transfer, the dimension is:\", X.shape[1], \"\\n\",\n",
    "      \"After transfer, the dimension is:\", X2D.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7736780290179156"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S2: Finding the \"explained variance\", or the information kept after transfer\n",
    "\n",
    "pca_.explained_variance_ratio_\n",
    "\n",
    "# \n",
    "\n",
    "np.sum(pca_.explained_variance_ratio_)\n",
    "\n",
    "# keep 99% of total info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.31450539,  0.28175124, -0.28007641, -0.30537643, -0.18807944,\n",
       "         0.24354445,  0.13436827,  0.13587422,  0.2229221 ,  0.27520915,\n",
       "        -0.24397436, -0.31450539, -0.18922935, -0.15983627, -0.10756097,\n",
       "        -0.22627007, -0.09237636, -0.22692684, -0.22533772],\n",
       "       [ 0.03469199, -0.17726021,  0.0145081 ,  0.13551239, -0.16114449,\n",
       "        -0.24322817,  0.41415506,  0.36312139, -0.39346786, -0.24928609,\n",
       "        -0.13269506,  0.03469199,  0.12028806, -0.07355431, -0.18455983,\n",
       "        -0.25464323,  0.24768088, -0.26641181, -0.27095034],\n",
       "       [ 0.04658176,  0.17586645,  0.09289558, -0.10373272,  0.25745498,\n",
       "         0.26028364,  0.45837217,  0.49984064, -0.12487963, -0.06039119,\n",
       "        -0.06453794,  0.04658176, -0.03144965,  0.05373338,  0.34788174,\n",
       "         0.15319886, -0.30251196,  0.20708172,  0.21258742],\n",
       "       [-0.24240499, -0.18892872, -0.33688527, -0.06970064, -0.52481814,\n",
       "        -0.21067488,  0.05044269,  0.02281774, -0.14814286, -0.03216107,\n",
       "        -0.15822957, -0.24240499,  0.12647012,  0.0550274 ,  0.07606184,\n",
       "         0.28007955,  0.04235289,  0.3556332 ,  0.35498449]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_2.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xnew = x1 -0.31450539+ x2  0.28175124+ x3 -0.28007641+ x4 -0.30537643"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yes, because we did scaling, now the coeffs are much more smaller in value and the difference of scale among\n",
    "#the features is not problematic anymore but we can see the sign of them(+,-) are the same, wich means the eigen vector\n",
    "#covariace matrix is scaled some hoe and the positive or negative relationship between\n",
    "#s new and other four components are the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yes, without scaling the variance was higher and that was the problem that the scale of the features were\n",
    "#different and it is not the true explained variance ratio for the unscaled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kurt has very big numbers and this can affect our pca var exp ratio alot but scaling can fix this problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
